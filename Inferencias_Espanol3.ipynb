{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Inferencias Espanol3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Metaboll/Nano-Robi/blob/master/Inferencias_Espanol3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSnkZshjps3Y",
        "colab_type": "code",
        "outputId": "7df9f515-4f54-4b30-99c5-6efea5c5e489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 8/4/2020\n",
        "# Version de Spacy es  2.2.4 (anterior 2019 ---> spacy==2.0.18)\n",
        "\n",
        "# things we need for NLP\n",
        "#https://towardsdatascience.com/build-it-yourself-chatbot-api-with-keras-tensorflow-model-f6d75ce957a5\n",
        "# json EDITOR\n",
        "\n",
        "# EJEMPLOS SPACY\n",
        "#https://spacy.io/usage/examples\n",
        "\n",
        "# tree para spacy\n",
        "#https://stackoverflow.com/questions/39323325/can-i-find-subject-from-spacy-dependency-tree-using-nltk-in-python\n",
        "\n",
        "\n",
        "#\n",
        "#https://jsoneditoronline.org/\n",
        "#\n",
        "#spacy==2.0.18\n",
        "#https://github.com/explosion/spaCy/issues/3451\n",
        "\n",
        "# ***************** CUANDO TENIA PROBLEMAS CON LA RASPBERRY Y SPACY\n",
        "#!pip install -U spacy==2.0.18\n",
        "#!python -m spacy download en_core_web_sm\n",
        "\n",
        "# EN ESPAÑOL\n",
        "!pip install -U spacy\n",
        "print(\"***\"*10)\n",
        "!python -m spacy download es_core_news_sm\n",
        "!python -m nltk.downloader all\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "import spacy\n",
        "print(\" Version de Spacy es \", spacy.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Version de Spacy es  2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdjxUA4N7UlG",
        "colab_type": "code",
        "outputId": "01b3bb5c-5f8a-46de-f35c-16fa655a9ead",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.38.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434671 sha256=7ee9ad8ef8e2ec21a6eca0eb70130711dd7d5e6583ca41c92110d6a05f88a698\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLI-ew6D4g6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#INSTRUCCIONES\n",
        "\n",
        "!pip uninstall PyYAML -y\n",
        "!pip install chatterbot\n",
        "!pip install chatterbot_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBj0Uc1Q30re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "\n",
        "\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "# things we need for Tensorflow\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR_aVj4Cp0qN",
        "colab_type": "code",
        "outputId": "ff62582e-f4f3-4b5b-d90d-e5d1ffc7013d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxzwPgelqFyf",
        "colab_type": "code",
        "outputId": "2a68ae58-1fe4-404f-e443-6b63d1f5442d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "with open('drive/My Drive/data/data/intents_generales.json') as json_data: #intents_generales.json // intents_preguntas.json //intents_generales #intents_spanish_1\n",
        "    intents = json.load(json_data)\n",
        "\n",
        "print(\"Everything seems Ok\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Everything seems Ok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arwGFobJps3n",
        "colab_type": "code",
        "outputId": "8e55c971-8dd3-49a7-e567-446fcb3d7bf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?']\n",
        "# loop through each sentence in our intents patterns\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenize each word in the sentence\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        # add to our words list\n",
        "        words.extend(w)\n",
        "        # add to documents in our corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "\n",
        "# stem and lower each word and remove duplicates\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# sort classes\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "# documents = combination between patterns and intents\n",
        "print (len(documents), \"documents\")\n",
        "print(documents[0])\n",
        "print(documents[100])\n",
        "# classes = intents\n",
        "print (len(classes), \"classes\", classes)\n",
        "# words = all words, vocabulary\n",
        "print (len(words), \"unique stemmed words\", words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "322 documents\n",
            "(['Hola'], 'Saludos')\n",
            "(['La', 'moto', 'es', 'muy', 'rapida'], 'Ser')\n",
            "11 classes ['Adios', 'Duda', 'Esta', 'Gracias', 'Insultos', 'Peticionayuda', 'Preguntas', 'Saludos', 'Ser', 'matematicas', 'orden']\n",
            "444 unique stemmed words ['1', '10', '100', '1000', '2', '3', '4', '5', '6', '68', '7+8', 'a', 'abajo', 'abr', 'acero', 'adio', 'ag', 'agil', 'aguil', 'al', 'alt', 'altisimo', 'alto', 'amarillo', 'amorfo', 'an', 'anillo', 'apag', 'aplicac', 'aplicación', 'armando', 'armario', 'arreglado', 'av', 'avisp', 'ayud', 'ayudarm', 'ayudarno', 'azuc', 'año', 'bajito', 'bajo', 'barato', 'barco', 'basur', 'bien', 'bitch', 'blanca', 'blanco', 'bonit', 'bonito', 'borr', 'bril', 'buena', 'bueno', 'cabron', 'cal', 'cam', 'capit', 'caro', 'carrer', 'cas', 'chin', 'ciento', 'cinco', 'ciudad', 'claud', 'coch', 'cocin', 'colegio', 'comid', 'como', 'comodo', 'compar', 'compr', 'contestart', 'continu', 'cop', 'cordob', 'corrupto', 'cort', 'cos', 'coseno', 'criad', 'cuadrad', 'cual', 'cuando', 'cuant', 'cuanto', 'cuarto', 'cuatro', 'culo', 'culp', 'cunto', 'da', 'dam', 'de', 'deber', 'deberí', 'decim', 'decir', 'decirt', 'dejab', 'dejenerado', 'del', 'delgado', 'demasiad', 'denso', 'dentro', 'desast', 'desconect', 'deseleccion', 'desgraciado', 'despacido', 'despacio', 'despistad', 'desprecy', 'destornillad', 'detuvo', 'dia', 'dim', 'diriget', 'disfraz', 'distanc', 'divid', 'dividido', 'doc', 'dond', 'dos', 'dud', 'dudo', 'edad', 'el', 'elen', 'elevado', 'ell', 'ello', 'embarazad', 'embarqu', 'empiez', 'en', 'encim', 'encontrast', 'encuentra', 'enferm', 'enfermo', 'enjendro', 'enmedio', 'entiendo', 'er', 'es', 'esa', 'escod', 'escondido', 'escuch', 'escucho', 'eso', 'espabilado', 'espejo', 'esper', 'espera', 'esperando', 'est', 'esta', 'estab', 'estado', 'estamo', 'estrecho', 'estrict', 'estuch', 'estuv', 'estuvo', 'excesiba', 'extraño', 'extremada', 'fachad', 'falt', 'fea', 'feo', 'fernando', 'figur', 'filtr', 'fin', 'finalizast', 'flaco', 'fol', 'follado', 'fras', 'freg', 'frio', 'fuck', 'fue', 'fuert', 'fuist', 'furgonet', 'gafa', 'garas', 'googl', 'gord', 'gordo', 'gracia', 'grand', 'grifo', 'grueso', 'guap', 'gust', 'ha', 'habitación', 'hac', 'has', 'hast', 'hay', 'hello', 'hemo', 'hermano', 'hermoso', 'hi', 'hij', 'hijo', 'hijoput', 'hol', 'hor', 'hueco', 'human', 'idiot', 'iendo', 'ig', 'impresor', 'inmens', 'insert', 'inutil', 'jef', 'jig', 'jilipolla', 'juguet', 'la', 'labado', 'lapicero', 'largo', 'las', 'lent', 'lento', 'lesbian', 'lesbin', 'libro', 'limpi', 'linto', 'llama', 'llav', 'lleg', 'llegado', 'llego', 'llegu', 'lo', 'loco', 'los', 'luego', 'maciz', 'mader', 'madr', 'madrid', 'mal', 'maleta', 'maletero', 'map', 'maricon', 'mas', 'may', 'me', 'med', 'medio', 'mej', 'men', 'meno', 'mes', 'mi', 'miedo', 'mierd', 'mis', 'morad', 'moreno', 'mosc', 'mot', 'moto', 'much', 'mucha', 'muevet', 'multiplicado', 'music', 'muy', 'más', 'nad', 'necesito', 'negro', 'no', 'noch', 'nombr', 'nos', 'nosotro', 'nuec', 'nuev', 'o', 'ojal', 'olvidam', 'oro', 'palo', 'pan', 'par', 'parast', 'parido', 'part', 'pas', 'pasillo', 'paso', 'patin', 'patinet', 'pedofilo', 'peg', 'peligros', 'pelirroj', 'pequeño', 'pervertido', 'piedr', 'pist', 'plac', 'plastico', 'plat', 'platino', 'poco', 'podria', 'polic', 'pon', 'por', 'potenc', 'precio', 'principio', 'problem', 'proceso', 'program', 'pronto', 'proporc', 'propuest', 'proxim', 'pued', 'puert', 'puerto', 'puntero', 'put', 'que', 'qued', 'qui', 'quieto', 'quint', 'quit', 'raiz', 'rapid', 'rapido', 'raro', 'real', 'reinic', 'reproduc', 'reproducct', 'respuest', 'resultado', 'retret', 'revovin', 'rie', 'robot', 'rojo', 'rom', 'romano', 'rop', 'rot', 'rub', 'rubio', 'ruidos', 'rus', 's', 'sab', 'salt', 'sana', 'sano', 'sar', 'se', 'seleccion', 'selección', 'seno', 'separ', 'ser', 'serio', 'sevill', 'señ', 'si', 'sido', 'sol', 'soluc', 'solución', 'son', 'soy', 'stop', 'suelo', 'sup', 'superintelig', 'supo', 'tan', 'tang', 'tanto', 'tard', 'te', 'telefono', 'ten', 'tengo', 'termin', 'terminado', 'terminast', 'tiempo', 'tien', 'todo', 'tonto', 'traduc', 'traducir', 'tre', 'triciclo', 'tu', 'turin', 'tuv', 'un', 'val', 'veloz', 'vemo', 'veneno', 'ventan', 'verd', 'vertedero', 'vestido', 'vet', 'vez', 'video', 'vist', 'viv', 'yo', 'york', 'youtub', 'zapatilla']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjUrzXk6ps3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create our training data\n",
        "training = []\n",
        "# create an empty array for our output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # stem each word - create base word, in attempt to represent related words\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "    # create our bag of words array with 1, if word match found in current pattern\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "    \n",
        "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    \n",
        "    training.append([bag, output_row])\n",
        "\n",
        "# shuffle our features and turn into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "# create train and test lists. X - patterns, Y - intents\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBzWJ_DopgAZ",
        "colab_type": "code",
        "outputId": "eee983d4-d950-4c49-b6a9-0bc24bc97532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#!pip install -U keras\n",
        "!pip install keras==2.3.1\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense, Activation, Dropout\n",
        "#from keras.optimizers import SGD\n",
        "from tensorflow.python.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.keras.optimizers import SGD\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.3.1 in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.18.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnityjfVps3z",
        "colab_type": "code",
        "outputId": "22338aa5-a596-466e-e4df-1ea9655c4d56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
        "# equal to number of intents to predict output intent with softmax\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "print(\"It's seems correctly\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It's seem all correctly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeFJKvk0ps31",
        "colab_type": "code",
        "outputId": "6c6710e3-3c9f-4e56-b892-f2e590d5bccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "print(\"paso\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "paso\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isB-k3vZps34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit the model\n",
        "model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-qy9ADhps37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # tokenize the pattern - split words into array\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stem each word - create short form for word\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "def bow(sentence, words, show_details=True):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # bag of words - matrix of N words, vocabulary matrix\n",
        "    bag = [0]*len(words)  \n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s: \n",
        "                # assign 1 if current word is in the vocabulary position\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "\n",
        "    return(np.array(bag))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxbxbAV7ps4A",
        "colab_type": "code",
        "outputId": "de0d629e-00f2-4573-8685-389d326d9796",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "p = bow(\"Adios\", words)\n",
        "print (p)\n",
        "print (classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "found in bag: adio\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "['Adios', 'Duda', 'Esta', 'Gracias', 'Insultos', 'Peticionayuda', 'Preguntas', 'Saludos', 'Ser', 'matematicas', 'orden']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD0fO7Uaps4C",
        "colab_type": "code",
        "outputId": "dfc0d28e-3dc5-4b62-a0f7-7faa3a79c6a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "p = bow(\"Ella fue muy alta\", words)\n",
        "#You are boring da problems\n",
        "#print (p)\n",
        "#print (classes)\n",
        "inputvar = pd.DataFrame([p], dtype=float, index=['input'])\n",
        "results = model.predict(inputvar)\n",
        "results_index = np.argmax(results)\n",
        "print(\" El numero de las etiquetas  que es solucion es \",  results_index)\n",
        "print(results[0][results_index])\n",
        "results[0][results_index] = 0\n",
        "tag = classes[results_index]\n",
        "print(tag)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " El numero de las etiquetas  que es solucion es  2\n",
            "0.99950516\n",
            "Siono\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxyQEWeVZKdi",
        "colab_type": "code",
        "outputId": "bfcbe560-5486-4e38-d998-650d266e60a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "#Probelma con ; Como te llamas\n",
        "while True:\n",
        "    message=input('You: ')\n",
        "    if message.strip() != 'Bye':\n",
        "        p = bow(message, words)\n",
        "        inputvar = pd.DataFrame([p], dtype=float, index=['input'])\n",
        "        results = model.predict(inputvar)\n",
        "        results_index = np.argmax(results)\n",
        "        #print(\" El numero de las etiquetas  que es solucion es \",  results_index)\n",
        "        print(\"Confianza del resultado >>>>>>\" ,results[0][results_index])\n",
        "        results[0][results_index] = 0\n",
        "        tag = classes[results_index]\n",
        "        print(\" El ambito de la conversacio´n es ; \", tag)\n",
        "        \n",
        "        \n",
        "    if message.strip() == 'Bye':\n",
        "        print('ChatBot : Bye')\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You: Como te llamas\n",
            "found in bag: como\n",
            "found in bag: te\n",
            "found in bag: llama\n",
            "Confianza del resultado >>>>>> 1.0\n",
            " El ambito de la conversacio´n es ;  quieneres\n",
            "You: que nombre tienes\n",
            "found in bag: que\n",
            "found in bag: nombr\n",
            "found in bag: tien\n",
            "Confianza del resultado >>>>>> 0.9999889\n",
            " El ambito de la conversacio´n es ;  quieneres\n",
            "You: Cuanto pesas\n",
            "found in bag: cuanto\n",
            "found in bag: pesa\n",
            "Confianza del resultado >>>>>> 0.99995565\n",
            " El ambito de la conversacio´n es ;  cuantopesa\n",
            "You: Que mides\n",
            "found in bag: que\n",
            "found in bag: mid\n",
            "Confianza del resultado >>>>>> 0.7478779\n",
            " El ambito de la conversacio´n es ;  quequieres\n",
            "You: Cuanta distancia hay entre malaga y sevilla\n",
            "found in bag: cuant\n",
            "found in bag: distanc\n",
            "found in bag: hay\n",
            "found in bag: ent\n",
            "Confianza del resultado >>>>>> 0.9997384\n",
            " El ambito de la conversacio´n es ;  cuantadistancia\n",
            "You: Cuanto tardas en llagar a casa\n",
            "found in bag: cuanto\n",
            "found in bag: tarda\n",
            "found in bag: en\n",
            "found in bag: a\n",
            "found in bag: cas\n",
            "Confianza del resultado >>>>>> 0.9999751\n",
            " El ambito de la conversacio´n es ;  tiempodeespera\n",
            "You: Eres alto\n",
            "found in bag: er\n",
            "Confianza del resultado >>>>>> 0.99998295\n",
            " El ambito de la conversacio´n es ;  Siono\n",
            "You: Has terminado los problemas\n",
            "found in bag: has\n",
            "found in bag: terminado\n",
            "found in bag: los\n",
            "Confianza del resultado >>>>>> 0.823515\n",
            " El ambito de la conversacio´n es ;  Siono\n",
            "You: Bye\n",
            "ChatBot : Bye\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggdu06nNps4G",
        "colab_type": "code",
        "outputId": "884f1272-57ff-477c-e6da-e80ce1e7d5a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# save model to file\n",
        "\n",
        "#pickle.dump(model, open(\"katana-assistant-model.pkl\", \"wb\"))\n",
        "#https://www.tensorflow.org/guide/keras/save_and_serialize\n",
        "\n",
        "model.save('model_keras.h5') # #model_keras /model_preguntas\n",
        "#new_model = model.load_model('path_to_my_model.h5')\n",
        "from tensorflow import keras\n",
        "\n",
        "mi_modelo = keras.models.load_model('model_keras.h5') #model_keras /model_preguntas"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xly2eIVps4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save all of our data structures\n",
        "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"datos_modelo.pkl\", \"wb\" ) ) #datos_modelo #info_modelo_preguntas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70-awEhnps4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pickle.load( open( \"katana-assistant-data.pkl\", \"rb\" ) )\n",
        "words = data['words']\n",
        "classes = data['classes']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZYXvDdm-2l1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputvar = pd.DataFrame([p], dtype=float, index=['input'])\n",
        "results = model.predict([bag_of_words(inp, words)])\n",
        "results_index = numpy.argmax(results)\n",
        "tag = labels[results_index]\n",
        "\n",
        "for tg in data[\"intents\"]:\n",
        "    if tg['tag'] == tag:\n",
        "        responses = tg['responses']\n",
        "\n",
        "        print(random.choice(responses))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhatlHrylvMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify_local(sentence):\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    \n",
        "    # generate probabilities from the model\n",
        "    input_data = pd.DataFrame([bow(sentence, words)], dtype=float, index=['input'])\n",
        "    results = mi_modelo.predict([input_data])[0]\n",
        "    # filter out predictions below a threshold, and provide intent index\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], str(r[1])))\n",
        "    # return tuple of intent and probability\n",
        "    \n",
        "    return return_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtM3v70jlzm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classify_local('En que parte de Madrid vives')\n",
        "#You are sooon boring\n",
        "#classify_local('Hello, good day!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ9PFrg43Ftr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhcWcQqZHxTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##/*********************************************************\n",
        "# C H A T T E R B O T "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGzrdZv-MqiZ",
        "colab_type": "code",
        "outputId": "4a913f2e-1bcd-4e67-8f60-15cde43b04c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!python -m chatterbot --version\n",
        "\n",
        "from chatterbot import ChatBot\n",
        "from chatterbot.trainers import ListTrainer\n",
        "#print(chatterbot.__version__)\n",
        "#!python chatterbot.show\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5B7LwvllUqG",
        "colab_type": "code",
        "outputId": "3d327c83-d2b0-46fc-c5f7-01a50be99411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "import os\n",
        "import spacy\n",
        "from google.colab import drive\n",
        "\n",
        "#from chatterbot.trainers import ListTrainer\n",
        "\n",
        "from chatterbot.trainers import ChatterBotCorpusTrainer\n",
        "\n",
        "bot=ChatBot('Bot')\n",
        "#bot.set_trainer(ListTrainer) #ERROR 'ChatBot' object has no attribute 'set_trainer'\n",
        "trainer = ChatterBotCorpusTrainer(bot)\n",
        "trainer.train(\"chatterbot.corpus.spanish\")\n",
        "\n",
        "print(\"Ready\")\n",
        "\n",
        "\n",
        "#****************************************************************\n",
        "#trainer = ListTrainer(chatbot)\n",
        "#trainer.train(conversation)\n",
        "\n",
        "\n",
        "# Create a new trainer for the chatbot\n",
        "#trainer = ChatterBotCorpusTrainer(chatbot)\n",
        "\n",
        "# Train the chatbot based on the english corpus\n",
        "#trainer.train(\"chatterbot.corpus.english\")\n",
        "\n",
        "# Get a response to an input statement\n",
        "#chatbot.get_response(\"Hello, how are you today?\")\n",
        "#********************************************************************\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#for files in os.listdir('D:/chatterbot-corpus-master/chatterbot_corpus/data/english/'):\n",
        "#with open( 'gdrive/My Drive/data/twitter/metadata.pkl', 'rb') as f:\n",
        "    #data= open('D:/chatterbot-corpus-master/chatterbot_corpus/data/english/'+files,'r').readlines()\n",
        "for files in os.listdir('drive/My Drive/data/spanish'):\n",
        "    print(\"files es : \" , files)\n",
        "    data= open('drive/My Drive/data/spanish/' + files,'r').readlines()\n",
        "    #bot.train(data)\n",
        "    trainer.train('drive/My Drive/data/spanish/' + files)\n",
        "    #trainer.train(\"data/trainingdata.yml\")\n",
        "\n",
        "while True:\n",
        "    message=input('You: ')\n",
        "    if message.strip() != 'Bye':\n",
        "        reply=bot.get_response(message)\n",
        "        #confi=bot.get_response(confidence)\n",
        "        #print(\"confi:\", confi)\n",
        "        print('ChatBot:',reply)\n",
        "    if message.strip() == 'Bye':\n",
        "        print('ChatBot : Bye')\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Training conversations.yml: [####################] 100%\n",
            "Training greetings.yml: [####################] 100%\n",
            "\n",
            "llego\n",
            "files es :  conversations.yml\n",
            "Training conversations.yml: [####################] 100%\n",
            "files es :  emociones.yml\n",
            "files es :  IA.yml\n",
            "Training IA.yml: [####################] 100%\n",
            "files es :  perfilbot\n",
            "\n",
            "files es :  trivia.yml\n",
            "\n",
            "files es :  greetings.yml\n",
            "Training greetings.yml: [####################] 100%\n",
            "files es :  psicologia.yml\n",
            "\n",
            "files es :  dinero.yml\n",
            "Training dinero.yml: [####################] 100%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-2ca9bbe1a9c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Bye'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mreply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F-ls_oTDAZ9",
        "colab_type": "code",
        "outputId": "ed0cb0ef-999c-4ca9-cb00-0d0e61b7c563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "import es_core_news_sm\n",
        "nlp = es_core_news_sm.load()\n",
        "print(\" Version de Spacy es \", spacy.__version__)\n",
        "print(\"1mera prueva\")\n",
        "\n",
        "doc = nlp(u'Facebook esta trabajando hasta el 15 de Abril')\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)\n",
        "while True:\n",
        "    message=input('You: ')\n",
        "    if message.strip() != 'Bye':\n",
        "        reply=bot.get_response(message)\n",
        "        print('ChatBot:',reply)\n",
        "    if message.strip() == 'Bye':\n",
        "        print('ChatBot : Bye')\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-683044262a5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mes_core_news_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mes_core_news_sm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Version de Spacy es \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1mera prueva\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/es_core_news_sm/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, **overrides)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;31m# language subclass) while keeping top-level language identifier \"lang\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lang_factory\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lang\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, **overrides)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# path to model data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mget_lang_class\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \"\"\"\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Check if language is registered / entry point is available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/lang/es/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_exceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBASE_EXCEPTIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_exceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBASE_NORMS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLANG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNORM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mupdate_exc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_lookups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munderscore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnderscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/tokenizer.cpython-36m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36minit spacy.tokenizer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'spacy.tokenizer.array' has no attribute '__reduce_cython__'"
          ]
        }
      ]
    }
  ]
}